{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105267,"databundleVersionId":12693789,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()) and \"KAGGLE_KERNEL_RUN_TYPE\" not in \"\".join(os.environ.keys()):\n    # Assuming local environment if not Colab or Kaggle\n    !pip install unsloth\nelif \"COLAB_\" in \"\".join(os.environ.keys()):\n    # Colab specific installations\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps git+https://github.com/huggingface/transformers.git # Only for Gemma 3N\n    !pip install --no-deps --upgrade timm \n    !pip install --no-deps unsloth\nelif \"KAGGLE_KERNEL_RUN_TYPE\" in \"\".join(os.environ.keys()):\n    # Kaggle specific installations (often similar to Colab for these packages)\n    !pip install --no-deps bitsandbytes accelerate peft trl==0.15.2 unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n    !pip install --no-deps git+https://github.com/huggingface/transformers.git # Only for Gemma 3N\n    !pip install --no-deps --upgrade timm \n    !pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:49:11.296213Z","iopub.execute_input":"2025-07-11T08:49:11.296524Z","iopub.status.idle":"2025-07-11T08:50:01.441766Z","shell.execute_reply.started":"2025-07-11T08:49:11.296499Z","shell.execute_reply":"2025-07-11T08:50:01.440963Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastModel\nimport torch\n\n# Check for GPU availability for Kaggle T4x2\nif not torch.cuda.is_available():\n    raise SystemError(\"GPU not available. Please ensure your Kaggle notebook is set up with a T4x2 GPU.\")\nif torch.cuda.device_count() < 2:\n    print(f\"Warning: Expected 2 GPUs for T4x2, but found {torch.cuda.device_count()}. Code will run but may not utilize full T4x2 potential if not configured for multi-GPU via SFTConfig/other means.\")\n\n\n# You can choose other Gemma-3 models from the fourbit_models list if needed.\n# For T4x2, gemma-3-4b-it is a good starting point.But we are going to 12b to see unsloth performance \n\nmodel_name = \"unsloth/gemma-3n-E2B-it\" \n\nmodel, tokenizer = FastModel.from_pretrained(\n    model_name = model_name,\n    max_seq_length = 2048, \n    load_in_4bit = True,   # 4 bit quantization to reduce memory\n    load_in_8bit = False,\n    full_finetuning = False,\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:50:01.443211Z","iopub.execute_input":"2025-07-11T08:50:01.443460Z","iopub.status.idle":"2025-07-11T08:51:50.042494Z","shell.execute_reply.started":"2025-07-11T08:50:01.443436Z","shell.execute_reply":"2025-07-11T08:51:50.041858Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-07-11 08:50:19.381730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752223819.741611      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752223819.840265      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.7.3: Fast Gemma3N patching. Transformers: 4.54.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c494f184c4b046789ff9fcd7e98e839f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/2.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fadfa60f12549efa6e457a7c7939be5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2728bb98d6f4efbb2d12d3a1e2d158b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/469M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8db3a28c760f4b4393d38ba7413b5b32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c41dc5401344a53bc41303459b1c9e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"996414ef62eb495c9cc1789dd35b35e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3526710f8e28458f855971676f3da9a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"646decdf98764535899f3a3c20c00cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fafa0e8d1c1f48e0bab2823f99772b9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e94b70ae87dd4c8c8bd4fb6c53089cfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ebdfff555eb4dce8ae1bc4a079ef75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cddc693c51f04bd39ff99843d725daf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04cbc6a9b8c04d7bb4f00008f6a451e5"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastModel.get_peft_model(\n    model,\n    finetune_vision_layers    = False, # Turn off for just text\n    finetune_language_layers  = True,  \n    finetune_attention_modules= True,  \n    finetune_mlp_modules      = True, \n    r = 8, # Larger = higher accuracy, but might overfit. 8 or 16 is a good start.\n    lora_alpha = 16, # Recommended lora_alpha = 2 * r\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:51:52.815748Z","iopub.execute_input":"2025-07-11T08:51:52.816359Z","iopub.status.idle":"2025-07-11T08:51:57.745199Z","shell.execute_reply.started":"2025-07-11T08:51:52.816332Z","shell.execute_reply":"2025-07-11T08:51:57.744631Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model.language_model` require gradients\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n# Apply the Gemma-3 chat template\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\",\n)\n\n# Load the customer support dataset\nfrom datasets import load_dataset\ndataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\"\ndataset = load_dataset(dataset_name, split = \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:52:02.044811Z","iopub.execute_input":"2025-07-11T08:52:02.045406Z","iopub.status.idle":"2025-07-11T08:52:03.954469Z","shell.execute_reply.started":"2025-07-11T08:52:02.045383Z","shell.execute_reply":"2025-07-11T08:52:03.953733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42531132ea6047cf82d945a86a1983c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(â€¦)t_Training_Dataset_27K_responses-v11.csv:   0%|          | 0.00/19.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8215bcd8081947df89c09784dae660e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/26872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4971e9614484c5f87f8028d617131ab"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:15.073868Z","iopub.execute_input":"2025-07-10T10:09:15.074139Z","iopub.status.idle":"2025-07-10T10:09:15.079913Z","shell.execute_reply.started":"2025-07-10T10:09:15.07412Z","shell.execute_reply":"2025-07-10T10:09:15.079398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define the formatting function for our specific dataset structure\n# We need to convert the 'instruction' and 'response' fields into the Gemma-3 chat format.\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    responses = examples[\"response\"]\n    texts = []\n    for instruction, response in zip(instructions, responses):\n        \n        messages = [\n            {\"role\": \"user\", \"content\": instruction},\n            {\"role\": \"model\", \"content\": response},\n        ]\n        # apply_chat_template will format this into a single string for the model\n        # add_generation_prompt = False because we are providing the full conversation for training\n        formatted_text = tokenizer.apply_chat_template(messages, tokenize = False, add_generation_prompt = False)\n        texts.append(formatted_text)\n    return { \"text\" : texts, }\n\ndataset = dataset.map(formatting_prompts_func, batched = True,) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:52:08.116187Z","iopub.execute_input":"2025-07-11T08:52:08.116794Z","iopub.status.idle":"2025-07-11T08:52:10.692517Z","shell.execute_reply.started":"2025-07-11T08:52:08.116770Z","shell.execute_reply":"2025-07-11T08:52:10.691953Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/26872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"293a6e2c763648b5922ccf7081dd2a8e"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    eval_dataset = None, # You can set up an evaluation split from your dataset if desired\n    args = SFTConfig(\n        dataset_text_field = \"text\", # Our new formatted text field\n        per_device_train_batch_size = 1, \n        gradient_accumulation_steps = 4, \n        warmup_steps = 10, # Increased slightly\n        # num_train_epochs = 1, # Set this for 1 full training run if you remove max_steps.\n        max_steps = 200, # Increase for more thorough training. For a real run, consider 500-2000+ or use num_train_epochs\n        learning_rate = 2e-4,\n        logging_steps = 5, \n        optim = \"adamw_8bit\", \n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        report_to = \"none\", # Set to \"wandb\" or \"tensorboard\" if you want to log metrics\n        # dataset_num_proc = 2, # Can speed up preprocessing for large datasets, ensure it works with your environment\n        # packing = True, # Enable packing for faster training, if your dataset and max_seq_length allow\n    ),\n)\n\n# This function modifies the labels so the model only learns to predict the assistant's (model's) responses.\n# The parts of the input corresponding to the user's prompt are masked out (set to -100).\nfrom unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<start_of_turn>user\\n\", # This should match the user prompt start in Gemma-3 template\n    response_part = \"<start_of_turn>model\\n\",   # This should match the model response start in Gemma-3 template\n)\n\n# You can inspect a data sample to see how it's formatted after applying the chat template and response masking\n# print(\"Checking an example from the training dataset:\")\n# example_index = 0 # choose an index\n# print(\"\\nOriginal Instruction:\")\n# print(dataset[example_index]['instruction'])\n# print(\"\\nOriginal Response:\")\n# print(dataset[example_index]['response'])\n# print(\"\\nTokenized Input IDs (excerpt):\")\n# print(trainer.train_dataset[example_index][\"input_ids\"][:50]) # Print first 50 token ids\n# print(\"\\nDecoded Input IDs:\")\n# print(tokenizer.decode(trainer.train_dataset[example_index][\"input_ids\"]))\n# print(\"\\nLabels (excerpt, -100 means token is masked for loss calculation):\")\n# print(trainer.train_dataset[example_index][\"labels\"][:50]) # Print first 50 labels\n# print(\"\\nDecoded Labels (masked parts are ignored by decode, pad tokens might appear as spaces):\")\n# print(tokenizer.decode([label_id if label_id != -100 else tokenizer.pad_token_id for label_id in trainer.train_dataset[example_index][\"labels\"]]).replace(tokenizer.pad_token, \"\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:52:14.828479Z","iopub.execute_input":"2025-07-11T08:52:14.829039Z","iopub.status.idle":"2025-07-11T08:52:27.996110Z","shell.execute_reply.started":"2025-07-11T08:52:14.829016Z","shell.execute_reply":"2025-07-11T08:52:27.995181Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=4):   0%|          | 0/26872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5710d70222d54aa39482772d6df6ad9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/26872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a574cbca10f46fa8af218e13220a21b"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")\n\ngpu_stats = torch.cuda.get_device_properties(1)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:12:21.373813Z","iopub.execute_input":"2025-07-10T10:12:21.374662Z","iopub.status.idle":"2025-07-10T10:12:21.381948Z","shell.execute_reply.started":"2025-07-10T10:12:21.37463Z","shell.execute_reply":"2025-07-10T10:12:21.380711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Starting training...\")\ntrainer_stats = trainer.train()\nprint(\"Training finished.\")\nprint(trainer_stats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T08:52:37.236497Z","iopub.execute_input":"2025-07-11T08:52:37.237178Z","iopub.status.idle":"2025-07-11T09:02:50.906803Z","shell.execute_reply.started":"2025-07-11T08:52:37.237147Z","shell.execute_reply":"2025-07-11T09:02:50.905998Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 26,872 | Num Epochs = 1 | Total steps = 100\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 10,567,680 of 5,450,005,952 (0.19% trained)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 08:10, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>6.633500</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>7.404400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>7.174800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>6.598700</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>2.473800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.936900</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>1.678400</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.447800</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>1.380200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.272900</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.219000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.149300</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.194600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.167700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.149000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.134100</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.092000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.021500</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.085900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.048600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training finished.\nTrainOutput(global_step=100, training_loss=2.46314651966095, metrics={'train_runtime': 611.181, 'train_samples_per_second': 1.309, 'train_steps_per_second': 0.164, 'total_flos': 2598874561808640.0, 'train_loss': 2.46314651966095})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Show final memory and time stats","metadata":{}},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:06:19.077079Z","iopub.execute_input":"2025-07-11T09:06:19.078857Z","iopub.status.idle":"2025-07-11T09:06:19.211253Z","shell.execute_reply.started":"2025-07-11T09:06:19.078821Z","shell.execute_reply":"2025-07-11T09:06:19.210330Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/828055189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Show final memory and time stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mused_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_memory_reserved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mused_memory_for_lora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_gpu_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mused_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_memory\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlora_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mused_memory_for_lora\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_memory\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'start_gpu_memory' is not defined"],"ename":"NameError","evalue":"name 'start_gpu_memory' is not defined","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"###  --- Inference ---","metadata":{}},{"cell_type":"code","source":"\n# Ensure the chat template is correctly re-applied for generation if it was modified\n# (it shouldn't be in this flow, but good practice if you were to change tokenizers)\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\", \n    # map_eos_token = True, # Gemma maps EOS to <end_of_turn>\n)\n\n# Test with a customer support type question\nmessages_support = [{\n    \"role\": \"user\",\n    \"content\": \"Why was I charged an extra fee this month on my bill?\"\n}]\ntext_support = tokenizer.apply_chat_template(\n    messages_support,\n    tokenize = False,\n    add_generation_prompt = True,\n)\nprint(f\"\\nFormatted prompt (Support): {text_support}\")\n\ninputs = tokenizer(\n    text=text_support, \n    return_tensors=\"pt\"\n).to(\"cuda\")\n\n# 2. Prepare the streamer\nfrom transformers import TextStreamer\nstreamer = TextStreamer(tokenizer, skip_prompt=True)\n\n# 3. Generate the response by unpacking the 'inputs' dictionary.\n#    This is cleaner and less prone to errors.\nprint(\"\\nModel Response:\")\n_ = model.generate(\n    **inputs, \n    max_new_tokens=512,\n    temperature=0.8,\n    top_p=0.95,\n    top_k=60,\n    streamer=streamer,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:06:25.948994Z","iopub.execute_input":"2025-07-11T09:06:25.949283Z","iopub.status.idle":"2025-07-11T09:07:16.989731Z","shell.execute_reply.started":"2025-07-11T09:06:25.949262Z","shell.execute_reply":"2025-07-11T09:07:16.989103Z"}},"outputs":[{"name":"stdout","text":"\nFormatted prompt (Support): <bos><start_of_turn>user\nWhy was I charged an extra fee this month on my bill?<end_of_turn>\n<start_of_turn>model\n\n\nModel Response:\nI'm sorry to hear that you were charged an extra fee this month on your bill. Let's investigate this together to understand the reason behind it. To provide you with an accurate explanation, I need some more information about the specific fee you were charged. Could you please provide the details of the fee, such as the amount and the date it was charged? Once I have this information, I can investigate the matter further and provide you with a clear explanation.<end_of_turn>\n\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"\n# Ensure the chat template is correctly re-applied for generation if it was modified\n# (it shouldn't be in this flow, but good practice if you were to change tokenizers)\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"gemma-3\", \n    # map_eos_token = True, # Gemma maps EOS to <end_of_turn>\n)\n\n# Test with a customer support type question\nmessages_support = [{\n    \"role\": \"user\",\n    \"content\": \"What are the steps to return an item I bought online?\"\n}]\ntext_support = tokenizer.apply_chat_template(\n    messages_support,\n    tokenize = False,\n    add_generation_prompt = True,\n)\nprint(f\"\\nFormatted prompt (Support): {text_support}\")\n\ninputs = tokenizer(\n    text=text_support, \n    return_tensors=\"pt\"\n).to(\"cuda\")\n\n# 2. Prepare the streamer\nfrom transformers import TextStreamer\nstreamer = TextStreamer(tokenizer, skip_prompt=True)\n\n# 3. Generate the response by unpacking the 'inputs' dictionary.\n#    This is cleaner and less prone to errors.\nprint(\"\\nModel Response:\")\n_ = model.generate(\n    **inputs, \n    max_new_tokens=512,\n    temperature=0.8,\n    top_p=0.95,\n    top_k=60,\n    streamer=streamer,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:30:27.26325Z","iopub.execute_input":"2025-07-10T10:30:27.263855Z","iopub.status.idle":"2025-07-10T10:32:16.748875Z","shell.execute_reply.started":"2025-07-10T10:30:27.263828Z","shell.execute_reply":"2025-07-10T10:32:16.748083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%mkdir ../temp\n%cd /kaggle/temp","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:13:32.639753Z","iopub.execute_input":"2025-07-11T09:13:32.640495Z","iopub.status.idle":"2025-07-11T09:13:32.885455Z","shell.execute_reply.started":"2025-07-11T09:13:32.640464Z","shell.execute_reply":"2025-07-11T09:13:32.884510Z"}},"outputs":[{"name":"stdout","text":"/kaggle/temp\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"Gemma3\")\nlogin(token=hf_token)\nmodel.push_to_hub_merged(\"Arnic/gemma-3n-E2B-it_customer_support_QA_Unsloth\", tokenizer, save_method = \"merged_16bit\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:15:51.362943Z","iopub.execute_input":"2025-07-11T09:15:51.363231Z","iopub.status.idle":"2025-07-11T09:19:17.205276Z","shell.execute_reply.started":"2025-07-11T09:15:51.363211Z","shell.execute_reply":"2025-07-11T09:19:17.204695Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c16e55359d48c8922e97f8a9ffade1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9536c779efd420fb70e6fa6b093f151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f60339196f684945b4c1983f76c451ae"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\nCache check failed: model-00001-of-00003.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nDownloading safetensors index for unsloth/gemma-3n-e2b-it...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5735f96b53c4a47b265d57a85b244f9"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:   0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07302562acdf486e9df039e7ac9770d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2046ca5368c94b4cb237fc73ad156d13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a048645cc2844aa6b784c221dfaed89d"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:52<01:45, 52.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6936c0b09993486bb4ca6ea804b95a0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e12c192bf2648d28c801e813c6d24dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b6778e521841e087824c7640bd7e2e"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [02:15<01:10, 70.27s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12ed8183bd5406ca2ba6eca404cf36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc2a8372d05498f8ad51cf59342f8e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f2079f2c39e4b91946adbdfd9cc4ae0"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [03:06<00:00, 62.30s/it]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"model.save_pretrained_merged(\"gemma-3N-finetune\", tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:29:58.474645Z","iopub.execute_input":"2025-07-11T09:29:58.475471Z","iopub.status.idle":"2025-07-11T09:31:54.415372Z","shell.execute_reply.started":"2025-07-11T09:29:58.475445Z","shell.execute_reply":"2025-07-11T09:31:54.414566Z"}},"outputs":[{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\nCache check failed: model-00001-of-00003.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nDownloading safetensors index for unsloth/gemma-3n-e2b-it...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82e352d276404769b7621de5dd64ef8b"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:   0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6260d93133a450b9ef11fe9799d0d42"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:33<01:07, 33.83s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b6898d4ef842fbb6e091146b1b4bed"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:24<00:43, 43.78s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/2.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe70d266137e47389315ff6cbb2d3da0"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:48<00:00, 36.20s/it]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"\"\"\"model.save_pretrained_gguf(\n        \"gemma-3N-finetune\",\n        quantization_type = \"Q8_0\", # For now only Q8_0, BF16, F16 supported\n    )\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:41:22.122833Z","iopub.execute_input":"2025-07-11T09:41:22.123339Z","iopub.status.idle":"2025-07-11T09:47:52.849411Z","shell.execute_reply.started":"2025-07-11T09:41:22.123313Z","shell.execute_reply":"2025-07-11T09:47:52.846835Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e577b972cd574973ae6bc14897ad85a9"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69c0e64838cd4722998eddda51c4b53a"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2291425486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.save_pretrained_gguf(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;34m\"gemma-3N-finetune\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mquantization_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Q8_0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# For now only Q8_0, BF16, F16 supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36msave_to_gguf_generic\u001b[0;34m(model, save_directory, quantization_type, repo_id, token)\u001b[0m\n\u001b[1;32m   2253\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m     metadata = _convert_to_gguf(\n\u001b[0m\u001b[1;32m   2256\u001b[0m         \u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m         \u001b[0mprint_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/llama_cpp.py\u001b[0m in \u001b[0;36mconvert_to_gguf\u001b[0;34m(input_folder, output_filename, quantization_type, max_shard_size, print_output, print_outputs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsloth: Failed to convert {conversion_filename} to GGUF.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mprinted_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Unsloth: Failed to convert llama.cpp/unsloth_convert_hf_to_gguf.py to GGUF."],"ename":"RuntimeError","evalue":"Unsloth: Failed to convert llama.cpp/unsloth_convert_hf_to_gguf.py to GGUF.","output_type":"error"}],"execution_count":22},{"cell_type":"code","source":"\"\"\"model.push_to_hub_gguf(\n        \"gemma-3N-finetune\",\n        quantization_type = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n        repo_id = \"Arnic/gemma-3n-E2B-it_customersupport_Unsloth-gguf\",\n        \n    )\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-11T09:52:00.604205Z","iopub.execute_input":"2025-07-11T09:52:00.604809Z","iopub.status.idle":"2025-07-11T09:58:45.319850Z","shell.execute_reply.started":"2025-07-11T09:52:00.604782Z","shell.execute_reply":"2025-07-11T09:58:45.317336Z"}},"outputs":[{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e6a66bfb064aa08f206d08cbcc3c60"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: GGUF conversion:   0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1083f02f8ac5420f8d512d52d7d15c9a"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3231149856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.push_to_hub_gguf(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0;34m\"gemma-3N-finetune\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mquantization_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Q8_0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Only Q8_0, BF16, F16 supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Arnic/gemma-3n-E2B-it_customersupport_Unsloth-gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36msave_to_gguf_generic\u001b[0;34m(model, save_directory, quantization_type, repo_id, token)\u001b[0m\n\u001b[1;32m   2253\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m     metadata = _convert_to_gguf(\n\u001b[0m\u001b[1;32m   2256\u001b[0m         \u001b[0msave_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m         \u001b[0mprint_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth_zoo/llama_cpp.py\u001b[0m in \u001b[0;36mconvert_to_gguf\u001b[0;34m(input_folder, output_filename, quantization_type, max_shard_size, print_output, print_outputs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsloth: Failed to convert {conversion_filename} to GGUF.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mprinted_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Unsloth: Failed to convert llama.cpp/unsloth_convert_hf_to_gguf.py to GGUF."],"ename":"RuntimeError","evalue":"Unsloth: Failed to convert llama.cpp/unsloth_convert_hf_to_gguf.py to GGUF.","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"\n#model.push_to_hub(\"Arnic/gemma-3n-E2B-it_customersupport_Unsloth\") # Online saving\n#tokenizer.push_to_hub(\"Arnic/gemma-3n-E2B-it_customersupport_Unsloth\") # Online saving","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}