{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8354915,"sourceType":"datasetVersion","datasetId":4964416}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-09T13:34:48.443442Z","iopub.execute_input":"2024-05-09T13:34:48.444261Z","iopub.status.idle":"2024-05-09T13:35:04.048008Z","shell.execute_reply.started":"2024-05-09T13:34:48.444231Z","shell.execute_reply":"2024-05-09T13:35:04.046892Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.2.4-py3-none-any.whl.metadata (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.3.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.2.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (3.6)\nDownloading lightning-2.2.4-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.4\n","output_type":"stream"}]},{"cell_type":"code","source":"import os.path as op\n\nfrom datasets import load_dataset\n\nimport lightning as L\nfrom lightning.pytorch.loggers import CSVLogger\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\nimport numpy as np\nimport pandas as pd\nimport torch\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:35:04.049824Z","iopub.execute_input":"2024-05-09T13:35:04.050096Z","iopub.status.idle":"2024-05-09T13:35:07.383825Z","shell.execute_reply.started":"2024-05-09T13:35:04.050071Z","shell.execute_reply":"2024-05-09T13:35:07.383010Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport tarfile\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom packaging import version\nfrom torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport urllib\n\n\ndef reporthook(count, block_size, total_size):\n    global start_time\n    if count == 0:\n        start_time = time.time()\n        return\n    duration = time.time() - start_time\n    progress_size = int(count * block_size)\n    speed = progress_size / (1024.0**2 * duration)\n    percent = count * block_size * 100.0 / total_size\n\n    sys.stdout.write(\n        f\"\\r{int(percent)}% | {progress_size / (1024.**2):.2f} MB \"\n        f\"| {speed:.2f} MB/s | {duration:.2f} sec elapsed\"\n    )\n    sys.stdout.flush()\n\n\ndef download_dataset():\n    source = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n    target = \"aclImdb_v1.tar.gz\"\n\n    if os.path.exists(target):\n        os.remove(target)\n\n    if not os.path.isdir(\"aclImdb\") and not os.path.isfile(\"aclImdb_v1.tar.gz\"):\n        urllib.request.urlretrieve(source, target, reporthook)\n\n    if not os.path.isdir(\"aclImdb\"):\n\n        with tarfile.open(target, \"r:gz\") as tar:\n            tar.extractall()\n\n\ndef load_dataset_into_to_dataframe():\n    basepath = \"aclImdb\"\n\n    labels = {\"pos\": 1, \"neg\": 0}\n\n    df = pd.DataFrame()\n\n    with tqdm(total=50000) as pbar:\n        for s in (\"test\", \"train\"):\n            for l in (\"pos\", \"neg\"):\n                path = os.path.join(basepath, s, l)\n                for file in sorted(os.listdir(path)):\n                    with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as infile:\n                        txt = infile.read()\n\n                    if version.parse(pd.__version__) >= version.parse(\"1.3.2\"):\n                        x = pd.DataFrame(\n                            [[txt, labels[l]]], columns=[\"review\", \"sentiment\"]\n                        )\n                        df = pd.concat([df, x], ignore_index=False)\n\n                    else:\n                        df = df.append([[txt, labels[l]]], ignore_index=True)\n                    pbar.update()\n    df.columns = [\"text\", \"label\"]\n\n    np.random.seed(0)\n    df = df.reindex(np.random.permutation(df.index))\n\n    print(\"Class distribution:\")\n    np.bincount(df[\"label\"].values)\n\n    return df\n\n\ndef partition_dataset(df):\n    df_shuffled = df.sample(frac=1, random_state=1).reset_index()\n\n    df_train = df_shuffled.iloc[:35_000]\n    df_val = df_shuffled.iloc[35_000:40_000]\n    df_test = df_shuffled.iloc[40_000:]\n\n    df_train.to_csv(\"train.csv\", index=False, encoding=\"utf-8\")\n    df_val.to_csv(\"val.csv\", index=False, encoding=\"utf-8\")\n    df_test.to_csv(\"test.csv\", index=False, encoding=\"utf-8\")\n\n\nclass IMDBDataset(Dataset):\n    def __init__(self, dataset_dict, partition_key=\"train\"):\n        self.partition = dataset_dict[partition_key]\n\n    def __getitem__(self, index):\n        return self.partition[index]\n\n    def __len__(self):\n        return self.partition.num_rows","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:35:07.385049Z","iopub.execute_input":"2024-05-09T13:35:07.385610Z","iopub.status.idle":"2024-05-09T13:35:07.404837Z","shell.execute_reply.started":"2024-05-09T13:35:07.385584Z","shell.execute_reply":"2024-05-09T13:35:07.403885Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"download_dataset()\n\ndf = load_dataset_into_to_dataframe()\npartition_dataset(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:35:07.406837Z","iopub.execute_input":"2024-05-09T13:35:07.407155Z","iopub.status.idle":"2024-05-09T13:36:34.025593Z","shell.execute_reply.started":"2024-05-09T13:35:07.407124Z","shell.execute_reply":"2024-05-09T13:36:34.024539Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"100% | 80.23 MB | 5.04 MB/s | 15.93 sec elapsed","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50000/50000 [00:47<00:00, 1044.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Class distribution:\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train = pd.read_csv(\"train.csv\")\ndf_val = pd.read_csv(\"val.csv\")\ndf_test = pd.read_csv(\"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:53:35.171456Z","iopub.execute_input":"2024-05-09T13:53:35.172133Z","iopub.status.idle":"2024-05-09T13:53:35.825753Z","shell.execute_reply.started":"2024-05-09T13:53:35.172104Z","shell.execute_reply":"2024-05-09T13:53:35.824731Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"imdb_dataset = load_dataset(\n    \"csv\",\n    data_files={\n        \"train\": \"train.csv\",\n        \"validation\": \"val.csv\",\n        \"test\": \"test.csv\",\n    },\n)\n\nprint(imdb_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:53:36.276681Z","iopub.execute_input":"2024-05-09T13:53:36.277041Z","iopub.status.idle":"2024-05-09T13:53:37.505997Z","shell.execute_reply.started":"2024-05-09T13:53:36.277004Z","shell.execute_reply":"2024-05-09T13:53:37.505170Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8218d55e3a34093b055a1ed79082641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b9c2330e8834a8f859f8b2e5a8f782a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5dcf154c0df43cd879b07a09fe5a8dd"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['index', 'text', 'label'],\n        num_rows: 35000\n    })\n    validation: Dataset({\n        features: ['index', 'text', 'label'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['index', 'text', 'label'],\n        num_rows: 10000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\nprint(\"Tokenizer input max length:\", tokenizer.model_max_length)\nprint(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:53:38.587515Z","iopub.execute_input":"2024-05-09T13:53:38.588260Z","iopub.status.idle":"2024-05-09T13:53:40.313355Z","shell.execute_reply.started":"2024-05-09T13:53:38.588229Z","shell.execute_reply":"2024-05-09T13:53:40.312445Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61ac5126d63b4e0f9ace60215393e791"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bad36b9c1e8d4195a99d06f1a9bb53cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb82866a3b1648dcb7e255916cb4deda"}},"metadata":{}},{"name":"stdout","text":"Tokenizer input max length: 512\nTokenizer vocabulary size: 30522\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_text(batch):\n    return tokenizer(batch[\"text\"], truncation=True, padding=True)\nimdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\ndel imdb_dataset\nimdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:53:41.717008Z","iopub.execute_input":"2024-05-09T13:53:41.717743Z","iopub.status.idle":"2024-05-09T13:54:14.360944Z","shell.execute_reply.started":"2024-05-09T13:53:41.717708Z","shell.execute_reply":"2024-05-09T13:54:14.359800Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/35000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79ea9bd4be2b4417ae21df8f5c7f4b40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a2a50b37704d78a96507661319b930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0d32ff120a45b0b21ab4370fdd484a"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\n\n\nclass IMDBDataset(Dataset):\n    def __init__(self, dataset_dict, partition_key=\"train\"):\n        self.partition = dataset_dict[partition_key]\n\n    def __getitem__(self, index):\n        return self.partition[index]\n\n    def __len__(self):\n        return self.partition.num_rows\n    \ntrain_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\nval_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\ntest_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=12,\n    shuffle=True, \n    num_workers=4\n)\n\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=12,\n    num_workers=4\n)\n\ntest_loader = DataLoader(\n    dataset=test_dataset,\n    batch_size=12,\n    num_workers=4\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:54:17.725529Z","iopub.execute_input":"2024-05-09T13:54:17.725858Z","iopub.status.idle":"2024-05-09T13:54:17.734384Z","shell.execute_reply.started":"2024-05-09T13:54:17.725832Z","shell.execute_reply":"2024-05-09T13:54:17.733264Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import lightning as L\nimport torch\nimport torchmetrics\n\n\nclass CustomLightningModule(L.LightningModule):\n    def __init__(self, model, learning_rate=5e-5):\n        super().__init__()\n\n        self.learning_rate = learning_rate\n        self.model = model\n\n        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n\n    def forward(self, input_ids, attention_mask, labels):\n        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n    def training_step(self, batch, batch_idx):\n        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                       labels=batch[\"label\"])        \n        self.log(\"train_loss\", outputs[\"loss\"])\n        return outputs[\"loss\"]  # this is passed to the optimizer for training\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                       labels=batch[\"label\"])        \n        self.log(\"val_loss\", outputs[\"loss\"], prog_bar=True)\n        \n        logits = outputs[\"logits\"]\n        predicted_labels = torch.argmax(logits, 1)\n        self.val_acc(predicted_labels, batch[\"label\"])\n        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n        \n    def test_step(self, batch, batch_idx):\n        outputs = self(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"],\n                       labels=batch[\"label\"])        \n        \n        logits = outputs[\"logits\"]\n        predicted_labels = torch.argmax(logits, 1)\n        self.test_acc(predicted_labels, batch[\"label\"])\n        self.log(\"accuracy\", self.test_acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:54:22.147843Z","iopub.execute_input":"2024-05-09T13:54:22.148533Z","iopub.status.idle":"2024-05-09T13:54:22.160860Z","shell.execute_reply.started":"2024-05-09T13:54:22.148497Z","shell.execute_reply":"2024-05-09T13:54:22.159911Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from lightning.pytorch.callbacks import ModelCheckpoint\nfrom lightning.pytorch.loggers import CSVLogger\n\n\ncallbacks = [\n    ModelCheckpoint(\n        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n    )  # save top 1 model\n]\nlogger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:54:29.644828Z","iopub.execute_input":"2024-05-09T13:54:29.645208Z","iopub.status.idle":"2024-05-09T13:54:29.656050Z","shell.execute_reply.started":"2024-05-09T13:54:29.645177Z","shell.execute_reply":"2024-05-09T13:54:29.655060Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2)\n\nlightning_model = CustomLightningModule(model)\ntrainer = L.Trainer(\n    max_epochs=3,\n    callbacks=callbacks,\n    accelerator=\"gpu\",\n    precision=\"16-mixed\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=100,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:54:31.893437Z","iopub.execute_input":"2024-05-09T13:54:31.894109Z","iopub.status.idle":"2024-05-09T13:54:32.733743Z","shell.execute_reply.started":"2024-05-09T13:54:31.894075Z","shell.execute_reply":"2024-05-09T13:54:32.732871Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nINFO: Using 16bit Automatic Mixed Precision (AMP)\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nstart = time.time()\n\ntrainer.fit(model=lightning_model,\n            train_dataloaders=train_loader,\n            val_dataloaders=val_loader)\n\nend = time.time()\nelapsed = end - start\nprint(f\"Time elapsed {elapsed/60:.2f} min\")\ntrainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")","metadata":{"execution":{"iopub.status.busy":"2024-05-09T13:54:36.632189Z","iopub.execute_input":"2024-05-09T13:54:36.632559Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"WARNING: Missing logger folder: logs/my-model\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name     | Type                                | Params\n-----------------------------------------------------------------\n0 | model    | DistilBertForSequenceClassification | 67.0 M\n1 | val_acc  | MulticlassAccuracy                  | 0     \n2 | test_acc | MulticlassAccuracy                  | 0     \n-----------------------------------------------------------------\n67.0 M    Trainable params\n0         Non-trainable params\n67.0 M    Total params\n267.820   Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1bcc4850ea49398ccb8fcf2a93da95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26718b014f5646db87458450c8ca0b54"}},"metadata":{}}]},{"cell_type":"markdown","source":"### **Last Layer**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2)\n\nlightning_model = CustomLightningModule(model)\nfor param in model.parameters():\n    param.requires_grad = False\n    \nfor param in model.classifier.parameters():\n    param.requires_grad = True\ntrainer = L.Trainer(\n    max_epochs=3,\n    callbacks=callbacks,\n    accelerator=\"gpu\",\n    precision=\"16-mixed\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=100,\n)\nimport time\nstart = time.time()\n\ntrainer.fit(model=lightning_model,\n            train_dataloaders=train_loader,\n            val_dataloaders=val_loader)\n\nend = time.time()\nelapsed = end - start\nprint(f\"Time elapsed {elapsed/60:.2f} min\")\ntrainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Last 2 Layers**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2)\n\nlightning_model = CustomLightningModule(model)\nfor param in model.parameters():\n    param.requires_grad = False\n    \nfor param in model.pre_classifier.parameters():\n    param.requires_grad = True\n    \nfor param in model.classifier.parameters():\n    param.requires_grad = True\ntrainer = L.Trainer(\n    max_epochs=3,\n    callbacks=callbacks,\n    accelerator=\"gpu\",\n    precision=\"16-mixed\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=100,\n)\nimport time\nstart = time.time()\n\ntrainer.fit(model=lightning_model,\n            train_dataloaders=train_loader,\n            val_dataloaders=val_loader)\n\nend = time.time()\nelapsed = end - start\nprint(f\"Time elapsed {elapsed/60:.2f} min\")\ntrainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Last 2 Layers & Last Tranformer Block**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2)\n\nlightning_model = CustomLightningModule(model)\nfor param in model.parameters():\n    param.requires_grad = False\n    \nfor param in model.pre_classifier.parameters():\n    param.requires_grad = True\n    \nfor param in model.classifier.parameters():\n    param.requires_grad = True\n\nfor param in model.distilbert.transformer.layer[5].parameters():\n    param.requires_grad = True\ntrainer = L.Trainer(\n    max_epochs=3,\n    callbacks=callbacks,\n    accelerator=\"gpu\",\n    precision=\"16-mixed\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=100,\n)\nimport time\nstart = time.time()\n\ntrainer.fit(model=lightning_model,\n            train_dataloaders=train_loader,\n            val_dataloaders=val_loader)\n\nend = time.time()\nelapsed = end - start\nprint(f\"Time elapsed {elapsed/60:.2f} min\")\ntrainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Last 2 Layers & Last 4 Transformer Blocks**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2)\n\nlightning_model = CustomLightningModule(model)\n## 6 -- Last 2 Layers + Last 4 Transformer Blocks\n\nfor param in model.parameters():\n    param.requires_grad = False\n    \nfor param in model.pre_classifier.parameters():\n    param.requires_grad = True\n    \nfor param in model.classifier.parameters():\n    param.requires_grad = True\n\nfor param in model.distilbert.transformer.layer[5].parameters():\n    param.requires_grad = True\n    \nfor param in model.distilbert.transformer.layer[4].parameters():\n    param.requires_grad = True\n    \nfor param in model.distilbert.transformer.layer[3].parameters():\n    param.requires_grad = True\n    \nfor param in model.distilbert.transformer.layer[2].parameters():\n    param.requires_grad = True\ntrainer = L.Trainer(\n    max_epochs=3,\n    callbacks=callbacks,\n    accelerator=\"gpu\",\n    precision=\"16-mixed\",\n    devices=1,\n    logger=logger,\n    log_every_n_steps=100,\n)\nimport time\nstart = time.time()\n\ntrainer.fit(model=lightning_model,\n            train_dataloaders=train_loader,\n            val_dataloaders=val_loader)\n\nend = time.time()\nelapsed = end - start\nprint(f\"Time elapsed {elapsed/60:.2f} min\")\ntrainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\")\ntrainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")","metadata":{},"execution_count":null,"outputs":[]}]}